# -*- coding: utf-8 -*-
"""Emotion_Detection_ComplexCNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hmHyqDTN5YeQteopzEZK0-vk0zDfikNy
"""

import os
import torch
import cv2
import numpy as np
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import re

import torch
import torch.nn as nn
import torch.nn.functional as F

from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, precision_score, recall_score
from sklearn.metrics import confusion_matrix, f1_score, roc_curve, auc

import matplotlib.pyplot as plt
import seaborn as sns

emotion_df = pd.read_csv('emotion_data.csv')

class VideoDataset(Dataset):
    def __init__(self, directory, emotion_df, transform=None, indices=None):
        super().__init__()
        self.directory = directory
        self.transform = transform
        self.emotion_df = emotion_df
        self.all_videos = self._load_video_paths()  # Load all videos initially
        self.videos = [self.all_videos[i] for i in indices] if indices is not None else self.all_videos

    def _load_video_paths(self):
        videos = []
        for root, dirs, files in os.walk(self.directory):
            video_files = [f for f in files if re.match(r'clip_\d+_frame_\d+\.png', f)]
            if video_files:
                try:
                    video_files = sorted(video_files, key=lambda x: (int(re.search(r'clip_(\d+)_frame_(\d+)\.png', x).group(1)),
                                                                     int(re.search(r'clip_(\d+)_frame_(\d+)\.png', x).group(2))))
                    video_id = os.path.basename(root)
                    emotion = self.emotion_df[self.emotion_df['Filename'].str.contains(video_id)]['Emotion'].values[0]
                    if len(video_files) > 6:
                        # Select 6 evenly spaced frames
                        indices = np.linspace(0, len(video_files) - 1, 6, dtype=int)
                        video_files = [video_files[i] for i in indices]
                    elif len(video_files) < 6:
                        # If fewer than 6, repeat the list until it has 6 elements
                        video_files = (video_files * ((6 // len(video_files)) + 1))[:6]
                    videos.append({
                        'path': root,
                        'frames': video_files,
                        'emotion': emotion - 1  # Convert emotion to 0-indexed
                    })
                except Exception as e:
                    print(f"Error processing files in {root}: {e}")
                    continue
        return videos

    def __len__(self):
        return len(self.videos)


    def __getitem__(self, idx):
        video_info = self.videos[idx]
        frames = []
        for filename in video_info['frames']:
            frame_path = os.path.join(video_info['path'], filename)
            frame = cv2.imread(frame_path)
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) if frame is not None else None
            if frame is not None and self.transform:
                frame = self.transform(frame)  # This should produce (C, H, W)
            frames.append(frame)

        # Stack frames along a new dimension (temporal dimension) at position 0
        frames = torch.stack(frames, dim=0)  # Now shape should be (T, C, H, W)

        # Rearrange dimensions to (C, T, H, W) for 3D ConvNet
        frames = frames.permute(1, 0, 2, 3)  # Permute dimensions to put channels first
        emotion_label = torch.tensor(video_info['emotion'], dtype=torch.long)
        return frames, emotion_label

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

full_dataset = VideoDataset(directory='drive/MyDrive/Data/Output', emotion_df=emotion_df, transform=transform)
video_loader = torch.utils.data.DataLoader(full_dataset, batch_size=4, shuffle=True)

# Split dataset into train, validation, and test sets
indices = list(range(len(full_dataset)))
train_indices, test_indices = train_test_split(indices, test_size=0.30, random_state=42)
val_indices, test_indices = train_test_split(test_indices, test_size=0.50, random_state=42)

train_dataset = VideoDataset(directory='drive/MyDrive/Data/Output', emotion_df=emotion_df, transform=transform, indices=train_indices)
val_dataset = VideoDataset(directory='drive/MyDrive/Data/Output', emotion_df=emotion_df, transform=transform, indices=val_indices)
test_dataset = VideoDataset(directory='drive/MyDrive/Data/Output', emotion_df=emotion_df, transform=transform, indices=test_indices)

# Create data loaders for each set
train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)

# class Complex3DCNN(nn.Module):
#     def __init__(self, num_classes=8):
#         super(Complex3DCNN, self).__init__()
#         self.conv1 = nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=1)
#         self.bn1 = nn.BatchNorm3d(64)
#         self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=1)
#         self.bn2 = nn.BatchNorm3d(128)
#         self.pool1 = nn.MaxPool3d((1, 2, 2))  # Adjusted pooling to not reduce depth

#         self.conv3 = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=1)
#         self.bn3 = nn.BatchNorm3d(256)
#         self.conv4 = nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=1)
#         self.bn4 = nn.BatchNorm3d(256)
#         self.pool2 = nn.MaxPool3d((1, 2, 2))  # Adjusted pooling to not reduce depth

#         self.conv5 = nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=1)
#         self.bn5 = nn.BatchNorm3d(512)
#         self.pool3 = nn.MaxPool3d((1, 2, 2))  # Adjusted pooling to not reduce depth

#         self.flatten = nn.Flatten()
#         # The dimensions here need to be recalculated based on the actual output of the last pooling layer
#         self.fc1 = nn.Linear(2408448, 1024)  # Adjusted based on actual output dimensions
#         self.dropout1 = nn.Dropout(0.5)
#         self.fc2 = nn.Linear(1024, num_classes)

#     def forward(self, x):
#         x = F.relu(self.bn1(self.conv1(x)))
#         x = self.pool1(F.relu(self.bn2(self.conv2(x))))
#         x = F.relu(self.bn3(self.conv3(x)))
#         x = self.pool2(F.relu(self.bn4(self.conv4(x))))
#         x = self.pool3(F.relu(self.bn5(self.conv5(x))))
#         x = self.flatten(x)
#         x = F.relu(self.fc1(x))
#         x = self.dropout1(x)
#         x = self.fc2(x)
#         return x

class Complex3DCNN(nn.Module):
    def __init__(self, num_classes=8):
        super(Complex3DCNN, self).__init__()
        self.conv1 = nn.Conv3d(3, 32, kernel_size=(3, 3, 3), padding=1)  # Reduced channels
        self.bn1 = nn.BatchNorm3d(32)
        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1)  # Reduced channels
        self.bn2 = nn.BatchNorm3d(64)
        self.pool1 = nn.MaxPool3d((1, 2, 2))

        self.conv3 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=1)  # Reduced channels
        self.bn3 = nn.BatchNorm3d(128)
        self.conv4 = nn.Conv3d(128, 128, kernel_size=(3, 3, 3), padding=1)  # Same as previous
        self.bn4 = nn.BatchNorm3d(128)
        self.pool2 = nn.MaxPool3d((1, 2, 2))

        self.conv5 = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=1)  # Reduced channels
        self.bn5 = nn.BatchNorm3d(256)
        self.pool3 = nn.MaxPool3d((1, 2, 2))

        self.flatten = nn.Flatten()
        # Adjust fc1 size based on new flattened size, should be recalculated
        self.fc1 = nn.Linear(1204224, 1024)
        self.dropout1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(1024, num_classes)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(F.relu(self.bn2(self.conv2(x))))
        x = F.relu(self.bn3(self.conv3(x)))
        x = self.pool2(F.relu(self.bn4(self.conv4(x))))
        x = self.pool3(F.relu(self.bn5(self.conv5(x))))
        x = self.flatten(x)
        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = self.fc2(x)
        return x

from torch.optim import Adam

def validate_model(model, dataloader, device):
    model.eval()
    all_preds = []
    all_labels = []
    val_loss = 0.0
    criterion = nn.CrossEntropyLoss()

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    val_loss /= len(dataloader)
    precision = precision_score(all_labels, all_preds, average='macro')
    recall = recall_score(all_labels, all_preds, average='macro')
    f1 = f1_score(all_labels, all_preds, average='macro')

    return val_loss, precision, recall, f1


def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.001):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    optimizer = Adam(model.parameters(), lr=learning_rate)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        train_loss = running_loss / len(train_loader)

        val_loss, precision, recall, f1 = validate_model(model, val_loader, device)
        print(f"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}")

    print("Training complete.")
    return model

# Train model
model = Complex3DCNN(num_classes=8)

trained_model = train_model(model, train_loader, val_loader, num_epochs=10)

torch.save(model.state_dict(), 'emotion_classification_model_complex.pth')

def evaluate_model(model, dataloader):
    # Check if CUDA is available and use GPU if it is, else use CPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)  # Ensure the model is on the correct device
    model.eval()

    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)  # Send inputs to the same device model is on
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Calculate F1 score
    f1 = f1_score(all_labels, all_preds, average='weighted')
    print(f'F1 Score: {f1}')

    # Plot Confusion Matrix
    cm = confusion_matrix(all_labels, all_preds)
    sns.heatmap(cm, annot=True, fmt='d')
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

    return f1, cm

test_accuracy = evaluate_model(trained_model, test_loader)
print("Test Accuracy:", test_accuracy)

from sklearn.preprocessing import label_binarize


def plot_multiclass_roc(model, dataloader, classes):
    # Check if CUDA is available and use GPU if it is, else use CPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)  # Ensure the model is on the correct device
    model.eval()

    y_test = []
    y_score = []

    # Process batches
    for inputs, labels in dataloader:
        inputs = inputs.to(device)
        labels = label_binarize(labels.numpy(), classes=[0, 1, 2, 3, 4, 5, 6, 7])  # Adjust classes based on your class labels
        y_test.extend(labels)

        outputs = model(inputs)
        y_score.extend(outputs.detach().cpu().numpy())

    y_test = np.array(y_test)
    y_score = np.array(y_score)

    # Compute ROC curve and ROC area for each class
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(len(classes)):
        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Plot ROC curve
    for i in range(len(classes)):
        plt.figure()
        plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Receiver Operating Characteristic for class {}'.format(classes[i]))
        plt.legend(loc="lower right")
        plt.show()

    return roc_auc

classes = [0, 1, 2, 3, 4, 5, 6, 7]

roc_auc = plot_multiclass_roc(trained_model, test_loader, classes)
print(roc_auc)

def get_metrics(model, dataloader):
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    all_preds = []
    all_labels = []
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Calculate metrics
    precision = precision_score(all_labels, all_preds, average=None)  # returns array for each class
    recall = recall_score(all_labels, all_preds, average=None)
    f1 = f1_score(all_labels, all_preds, average=None)

    return precision, recall, f1

def plot_classification_metrics(model, dataloader, class_names):
    precision, recall, f1 = get_metrics(model, dataloader)

    x = np.arange(len(class_names))  # the label locations
    width = 0.3  # the width of the bars

    fig, ax = plt.subplots(figsize=(12, 6))
    rects1 = ax.bar(x - width, precision, width, label='Precision')
    rects2 = ax.bar(x, recall, width, label='Recall')
    rects3 = ax.bar(x + width, f1, width, label='F1 Score')

    # Add some text for labels, title and custom x-axis tick labels, etc.
    ax.set_xlabel('Classes')
    ax.set_ylabel('Scores')
    ax.set_title('Precision, Recall and F1 Score by Class')
    ax.set_xticks(x)
    ax.set_xticklabels(class_names)
    ax.legend()

    # Add a legend and show the plot
    ax.legend()
    plt.show()

class_names = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']  # Replace with your actual class names
plot_classification_metrics(model, test_loader, class_names)

